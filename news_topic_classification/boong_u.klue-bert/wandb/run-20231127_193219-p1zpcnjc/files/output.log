
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[I 2023-11-27 19:38:05,510] Trial 0 finished with value: 0.8881831124739897 and parameters: {'learning_rate': 3.1616357881346564e-05, 'num_train_epochs': 5, 'seed': 10, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.8881831124739897.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.