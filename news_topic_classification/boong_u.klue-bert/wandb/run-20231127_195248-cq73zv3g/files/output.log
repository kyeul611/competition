
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[I 2023-11-27 19:54:29,632] Trial 0 finished with value: 0.8767933413645822 and parameters: {'learning_rate': 2.580269482323774e-06, 'num_train_epochs': 2, 'seed': 8, 'per_device_train_batch_size': 64}. Best is trial 0 with value: 0.8767933413645822.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.