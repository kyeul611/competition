
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[W 2023-11-27 19:50:54,700] Trial 0 failed with parameters: {'learning_rate': 5.236992151429448e-05, 'num_train_epochs': 1, 'seed': 4, 'per_device_train_batch_size': 4} because of the following error: KeyboardInterrupt().
Traceback (most recent call last):
  File "C:\Users\Kyeul\anaconda3\envs\nlp\Lib\site-packages\optuna\study\_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "C:\Users\Kyeul\anaconda3\envs\nlp\Lib\site-packages\transformers\integrations\integration_utils.py", line 195, in _objective
    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)
  File "C:\Users\Kyeul\anaconda3\envs\nlp\Lib\site-packages\transformers\trainer.py", line 1555, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kyeul\anaconda3\envs\nlp\Lib\site-packages\transformers\trainer.py", line 1860, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kyeul\anaconda3\envs\nlp\Lib\site-packages\transformers\trainer.py", line 2734, in training_step
    self.accelerator.backward(loss)
  File "C:\Users\Kyeul\anaconda3\envs\nlp\Lib\site-packages\accelerate\accelerator.py", line 1989, in backward
    loss.backward(**kwargs)
  File "C:\Users\Kyeul\anaconda3\envs\nlp\Lib\site-packages\torch\_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "C:\Users\Kyeul\anaconda3\envs\nlp\Lib\site-packages\torch\autograd\__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
