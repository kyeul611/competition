
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[I 2023-11-28 18:15:04,399] Trial 0 finished with value: 0.8961778556565546 and parameters: {'learning_rate': 6.931763037713002e-06, 'num_train_epochs': 5, 'seed': 23, 'per_device_train_batch_size': 64}. Best is trial 0 with value: 0.8961778556565546.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.