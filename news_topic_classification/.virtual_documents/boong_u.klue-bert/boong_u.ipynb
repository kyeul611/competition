


import os
import warnings

import numpy as np
import pandas as pd
import datasets
from datasets import load_metric
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from transformers import EarlyStoppingCallback

from sklearn.model_selection import train_test_split

import torch
from torch.utils.data import Dataset, DataLoader

import wandb

warnings.filterwarnings(action='ignore')





wandb.init(
    project='boong_u__klue-ynat_classification',
    entity='gyul611',
)


model_checkpoint = 'klue/bert-base'
batch_size=512+256
task='nli' # 자연어 추론
RANDOM_SEED=42


tokenizer=AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)


dataset = pd.read_csv("../data/train_data.csv")
test = pd.read_csv("../data/test_data.csv")


dataset_train, dataset_val = train_test_split(dataset, test_size=0.2, random_state=RANDOM_SEED)


class BERTDataset(Dataset):
    def __init__(self, dataset, sent_key, label_key, bert_tokenizer):
        self. sentences = [bert_tokenizer(i, truncation=True, return_token_type_ids=False) for i in dataset[sent_key]]

        # ? 왜 이렇게 하는 거지

        # label이 있으면 train 모드
        # 없으면 test 모드
        if not label_key == None:
            self.mode = "train"
        else:
            self.mode = "test"

        # label 매핑?
        if self.mode == "train":
            self.labels = [np.int64(i) for i in dataset[label_key]]
        else:
            self.labels = [np.int64(0) for i in dataset[sent_key]]

    def __getitem__(self, i):
        if self.mode == "train":
            self.sentences[i]['label'] = self.labels[i]
            return self.sentences[i]

        else:
            return self.sentences[i]

    def __len__(self):
        return (len(self.labels))


data_train = BERTDataset(dataset_train, "title", "topic_idx", tokenizer)
data_val = BERTDataset(dataset_val, "title", "topic_idx", tokenizer)
data_test = BERTDataset(test, "title", None, tokenizer)


num_labels = 7
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)


metric = load_metric("glue", "qnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


metric_name = "accuracy"

args = TrainingArguments(
    'test-nli',
    save_total_limit=5,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=10,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
    report_to='wandb',
)


# hyperparameter_search를 위해서는 model_init을 만들어야함.

def model_init():
    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)


trainer = Trainer(
    model_init=model_init,
    args=args,
    train_dataset=data_train,
    eval_dataset=data_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)



best_run = trainer.hyperparameter_search(n_trials=2, direction="maximize")


best_run


for n, v in best_run.hyperparameters.items():
    setattr(trainer.args, n, v)

trainer.train()






