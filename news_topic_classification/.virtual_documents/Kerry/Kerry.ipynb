


# load data
import pandas as pd
import matplotlib.pyplot as plt

train = pd.read_csv('../data/train_data.csv')
test = pd.read_csv('../data/test_data.csv')


train.head()


topic_dict = pd.read_csv("../data/topic_dict.csv")
topic_dict





train['topic_idx'].value_counts()


train['topic_idx'].value_counts(normalize=True)


train.isnull().sum()


train['title_length'] = train['title'].apply(lambda x: len(x))


train['title_length'].describe()


train.head()


train['title_length'].plot(kind='hist')


train[(train['title_length'] > 10) & (train['title_length'] < 15)]


# Main Tokenizer used in RobertaModel
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('klue/roberta-large')





from collections import Counter


# 한자 빈도수 확인
import re
k = []

for i in range(0, len(train)):
    a = re.findall('[一-龥]', train['title'][i])
    if len(a) !=0:
        k=[*k, *a]
Counter(k).most_common()[:15] # 각 원소의 갯수를 반환


len(k)


test = []

for i in range(4):
    a = [x for x in range(i)]
    print(a)
    test = [*test, *a] # list에 list 원소를 모두 넣음. 단 원소만!
    print(test)

Counter(test).most_common() # 각 원소의 중복되는 갯수를 보여줌/ value_counts()


# 한자를 한글로 치환

# index 저장
# replace할 때 반복문을 줄이기 위해?
hanja_idx = []
for i in range(0, len(train)):
    a = re.findall('[一-龥]', train['title'][i])
    
    if len(a)!=0:
        # print(a, i)
        hanja_idx.append(i)

# 치환
def replace_all(text, dic):
    for i, j in dic.items():
        text = text.replace(i, j)
    return text

d = { "中": "중국", "美": "미국","北":"북한",'日':"일본",'英':'영국','行':'행','靑':'청와대','朴':'박근혜','銀':'은행','與':'여당',
     '文':'문재인','野':'야당','獨':'독일','伊':'이탈리아','韓':'한국','佛':'프랑스','前':'전','檢':'검찰','軍':'군','安':'안철수','南':'남한',
     '亞':'아시아','展':'전시회','重':'차','株':'주식','詩':'시'}


train.iloc[hanja_idx, 1][:10]


re.sub(r'([一-龥])', r' \1 ', train.iloc[1, 1])


# 한자 앞뒤로 공백 추가
for idx in hanja_idx:
    train.iloc[idx, 1] = re.sub(r'([一-龥])', r' \1 ', train.iloc[idx, 1])


kr_title = []
for i in train.iloc[hanja_idx, 1]: # hanja_idx에 해당하는 title만 가져옴(1은 column 위치)
    temp = replace_all(i, d)
    kr_title.append(temp)
kr_title[:10]


# 한자-> 한글 치환 후 데이터 교체
for idx, kr_t in zip(hanja_idx, kr_title):
    train.iloc[idx, 1] = kr_t


train.iloc[hanja_idx, 1][:10]





train_data = train
train_1_2_3 = train_data[(train_data['topic_idx']==1) | (train_data['topic_idx']==2) | (train_data['topic_idx']==3)]
train_1_2_3_index = train_data[(train_data['topic_idx']==1) | (train_data['topic_idx']==2) | (train_data['topic_idx']==3)].index


train_1_2_3


len(train_1_2_3), len(train)


train_1_2_3_index


train_1_2_3['token_list'] = ''
train['token_list'] = ''
for i in train_1_2_3_index:
    token = tokenizer.tokenize(train_1_2_3.loc[i, 'title'])
    train_1_2_3.loc[i, 'token_list'] = ' '.join(token)
    train.loc[i, 'token_list'] = ' '.join(token)


train_1_2_3


# 유사도 계산
def return_similarity(a, b):
    c = a.intersection(b) # 교집합
    return float(len(c)) / (len(a) + len(b) - len(c))





# index가 1, 2, 3인 데이터의 토큰을 set으로 만들어서 중복 제거
# 이를 사용해 유사도를 구할 생각이다. 
set_train_log = [set(log.split()) for log in train['token_list']]


from tqdm import tqdm
# 유사도가 높은 문장을 위한 DataFrame


sim_idx = []
sim_i = []
sim_j = []
top_i = []
top_j = []

# 만약 유사도가 0.5으면서 레이블이 다르면, 이것은 유사한 문장이라고 봐야한다.
for i in tqdm(train_1_2_3_index):
    for j in train_1_2_3_index:
        if i == j:
            continue
        if ((return_similarity(set_train_log[i], set_train_log[j])) >= 0.5) and (train_1_2_3['topic_idx'][i]!=train_1_2_3['topic_idx'][j]):
            top_i.append(train_1_2_3['topic_idx'][i])
            top_j.append(train_1_2_3['topic_idx'][j])
            sim_i.append(i)
            sim_j.append(j)


# 의미적으로 같음에도 레이블이 다름

diff_label = pd.DataFrame({'sent1_idx': sim_i, 'sent2_idx': sim_j, 
                           'sent1' : train_1_2_3['title'][sim_i].values, 'sent2' : train_1_2_3['title'][sim_j].values, 
                           'top_1': top_i, 'top_2': top_j})
diff_label


import numpy as np
np.save('sim_1_2_3_0.5_klue_bert.npy', diff_label)






