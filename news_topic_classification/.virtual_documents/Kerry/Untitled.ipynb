


# load data
import pandas as pd

train = pd.read_csv('../data/train_data.csv')
test = pd.read_csv('../data/test_data.csv')


train.head()


# Main Tokenizer used in RobertaModel
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('klue/roberta-large')





from collections import Counter


# 한자 빈도수 확인
import re
k = []

for i in range(0, len(train)):
    a = re.findall('[一-龥]', train['title'][i])
    if len(a) !=0:
        k=[*k, *a]
Counter(k).most_common()[:15] # 각 원소의 갯수를 반환


len(k)


test = []

for i in range(4):
    a = [x for x in range(i)]
    print(a)
    test = [*test, *a]
    print(test)

Counter(test).most_common()


# 한자를 한글로 치환

# index 저장
# replace할 때 반복문을 줄이기 위해?
hanja_idx = []
for i in range(0, len(train)):
    a = re.findall('[一-龥]', train['title'][i])
    
    if len(a)!=0:
        # print(a, i)
        hanja_idx.append(i)

# 치환
def replace_all(text, dic):
    for i, j in dic.items():
        text = text.replace(i, j)
    return text

d = { "中": "중국", "美": "미국","北":"북한",'日':"일본",'英':'영국','行':'행','靑':'청와대','朴':'박근혜','銀':'은행','與':'여당',
     '文':'문재인','野':'야당','獨':'독일','伊':'이탈리아','韓':'한국','佛':'프랑스','前':'전','檢':'검찰','軍':'군','安':'안철수','南':'남한',
     '亞':'아시아','展':'전시회','重':'차','株':'주식','詩':'시'}


train.iloc[hanja_idx, 1][:10]


re.sub(r'([一-龥])', r' \1 ', train.iloc[1, 1])


(len(hanja_idx), len(kr_title))


# 한자 앞뒤로 공백 추가
for idx in hanja_idx:
    train.iloc[idx, 1] = re.sub(r'([一-龥])', r' \1 ', train.iloc[idx, 1])


kr_title = []
for i in train.iloc[hanja_idx, 1]: # hanja_idx에 해당하는 title만 가져옴(1은 column 위치)
    temp = replace_all(i, d)
    kr_title.append(temp)
kr_title[:10]


# 한자-> 한글 치환 후 데이터 교체
for idx, kr_t in zip(hanja_idx, kr_title):
    train.iloc[idx, 1] = kr_t


train.iloc[hanja_idx, 1][:10]





train_data = train
train_1_2_3 = train_data[(train_data['topic_idx']==1) | (train_data['topic_idx']==2) | (train_data['topic_idx']==3)]
train_1_2_3_index = train_data[(train_data['topic_idx']==1) | (train_data['topic_idx']==2) | (train_data['topic_idx']==3)].index


train_1_2_3_index


train_1_2_3['token_list'] = ''
train['token_list'] = ''
for i in train_1_2_3_index:
    token = tokenizer.tokenize(train_1_2_3['title'][i])
    train_1_2_3.iloc[i, -1] = ' '.join(token)
    train.iloc[i, -1] = ' '.join(token)


train_1_2_3


train_1_2_3.iloc[1, -1]



