


import os
import warnings

import numpy as np
import pandas as pd
import datasets
from datasets import load_metric
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

from sklearn.model_selection import train_test_split

import torch
from torch.utils.data import Dataset, DataLoader

warnings.filterwarnings(action='ignore')





model_checkpoint = 'klue/bert-base'
batch_size=32
task='nli' #?
RANDOM_SEED=42


tokenizer=AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)


dataset = pd.read_csv("../data/train_data.csv")
test = pd.read_csv("../data/test_data.csv")


dataset_train, dataset_val = train_test_split(dataset, test_size=0.2, random_state=RANDOM_SEED)


class BERTDataset(Dataset):
    def __init__(self, dataset, sent_key, label_key, bert_tokenizer):
        self. sentences = [bert_tokenizer(i, truncation=True, return_token_type_ids=False) for i in dataset[sent_key]]

        # ? 왜 이렇게 하는 거지

        # label이 있으면 train 모드
        # 없으면 test 모드
        if not label_key == None:
            self.mode = "train"
        else:
            self.mode = "test"

        # label 매핑?
        if self.mode == "train":
            self.labels = [np.int64(i) for i in dataset[label_key]]
        else:
            self.labels = [np.int64(0) for i in dataset[sent_key]]

    def __getitem__(self, i):
        if self.mode == "train":
            self.sentences[i]['label'] = self.labels[i]
            return self.sentences[i]

        else:
            return self.sentences[i]

    def __len__(self):
        return (len(self.labels))


data_train = BERTDataset(dataset_train, "title", "topic_idx", tokenizer)
data_val = BERTDataset(dataset_val, "title", "topic_idx", tokenizer)
data_test = BERTDataset(test, "title", None, tokenizer)


num_labels = 7
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)



