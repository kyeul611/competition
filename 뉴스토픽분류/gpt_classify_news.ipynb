{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, GPT2ForSequenceClassification\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./data_in/open/train_data.csv', index_col='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c152a1428e476baf25669b6a4bd45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/337 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52544c40a8f3483a8180aad1ab46bdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc187da60218443da6a6a16bd6b7c92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n",
      "Can't set hidden_size with value 1024 for GPT2Config {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.8.2\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fed2fe08f7d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"klue/roberta-large\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2ForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"klue/roberta-large\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[0;32m   1074\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m                 \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m             )\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0mreturn_unused_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return_unused_kwargs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pruned_heads\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\configuration_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_size, n_positions, n_ctx, n_embd, n_layer, n_head, n_inner, activation_function, resid_pdrop, embd_pdrop, attn_pdrop, layer_norm_epsilon, initializer_range, summary_type, summary_use_proj, summary_activation, summary_proj_to_labels, summary_first_dropout, scale_attn_weights, gradient_checkpointing, use_cache, bos_token_id, eos_token_id, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     ):\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Can't set {key} with value {value} for {self}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Can't set {key} with value {value} for {self}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert\")\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"klue/kobert\")\n",
    "model.score = torch.nn.Linear(768, 7)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_len=40):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.data.iloc[index]\n",
    "        document, label = str(record['title']), int(record['topic_idx'])\n",
    "        tokens = self.tokenizer.tokenize(document)\n",
    "        encoder_input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(encoder_input_id)\n",
    "        if len(encoder_input_id) < self.max_seq_len:\n",
    "            while len(encoder_input_id) < self.max_seq_len:\n",
    "                encoder_input_id += [tokenizer.convert_tokens_to_ids('<pad>')]\n",
    "                attention_mask += [0]\n",
    "        else:\n",
    "            encoder_input_id = encoder_input_id[:self.max_seq_len - 1] + [\n",
    "                self.tokenizer.eos_token_id]\n",
    "            attention_mask = attention_mask[:self.max_seq_len]\n",
    "        return {'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
    "                'attention_mask': np.array(attention_mask, dtype=np.float),\n",
    "                'labels': np.array(label, dtype=np.int_)}\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_len=40):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.data.iloc[index]\n",
    "        document = str(record['title'])\n",
    "        tokens = self.tokenizer.tokenize(document)\n",
    "        encoder_input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(encoder_input_id)\n",
    "        if len(encoder_input_id) < self.max_seq_len:\n",
    "            while len(encoder_input_id) < self.max_seq_len:\n",
    "                encoder_input_id += [tokenizer.convert_tokens_to_ids('<pad>')]\n",
    "                attention_mask += [0]\n",
    "        else:\n",
    "            encoder_input_id = encoder_input_id[:self.max_seq_len - 1] + [\n",
    "                self.tokenizer.eos_token_id]\n",
    "            attention_mask = attention_mask[:self.max_seq_len]\n",
    "        return {'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
    "                'attention_mask': np.array(attention_mask, dtype=np.float)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train parameters\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loader\n",
    "train_ds = TrainDataset(tr, tokenizer)\n",
    "loader = DataLoader(train_ds, batch_size=batch_size, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, )\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 798.4672225415707\n",
      "1 425.72672291472554\n",
      "2 216.4520208099857\n",
      "3 91.64503619493917\n",
      "4 37.197267876743354\n",
      "5 23.07639458939957\n",
      "6 27.07165218438513\n",
      "7 37.48703969925191\n",
      "8 68.31385060424645\n",
      "9 99.99077906967432\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        ids, atts, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        ids = torch.tensor(ids).long().cuda()\n",
    "        atts = torch.tensor(atts).long().cuda()\n",
    "        labels = torch.tensor(labels).long().cuda()\n",
    "        pred = model(ids, attention_mask=atts)\n",
    "        loss = loss_fn(pred[0], labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    scheduler.step()\n",
    "    print(e, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loader\n",
    "te = pd.read_csv('./data_in/open/test_data.csv', index_col='index')\n",
    "\n",
    "test_ds = TestDataset(te, tokenizer)\n",
    "test_loader = DataLoader(test_ds, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1142/1142 [00:14<00:00, 77.63it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "\n",
    "for b in tqdm(test_loader):\n",
    "    ids, atts = b['input_ids'], b['attention_mask']\n",
    "    ids = torch.tensor(ids).long().cuda()\n",
    "    atts = torch.tensor(atts).long().cuda()\n",
    "    pred = model(ids, attention_mask=atts)\n",
    "    preds += list(np.argmax(pred[0].detach().cpu().numpy(), 1))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45654</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45655</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45656</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45657</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45658</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45659</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45660</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45661</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45662</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45663</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45664</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45665</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45666</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45667</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45668</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45669</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45670</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45671</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45672</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45673</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_idx\n",
       "index           \n",
       "45654          3\n",
       "45655          3\n",
       "45656          0\n",
       "45657          2\n",
       "45658          3\n",
       "45659          2\n",
       "45660          5\n",
       "45661          3\n",
       "45662          4\n",
       "45663          4\n",
       "45664          4\n",
       "45665          6\n",
       "45666          4\n",
       "45667          6\n",
       "45668          6\n",
       "45669          1\n",
       "45670          4\n",
       "45671          2\n",
       "45672          4\n",
       "45673          4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('./data_in/open/sample_submission.csv', index_col='index')\n",
    "sub['topic_idx'] = preds\n",
    "sub.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('./gpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45654</th>\n",
       "      <td>유튜브 내달 2일까지 크리에이터 지원 공간 운영</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45655</th>\n",
       "      <td>어버이날 맑다가 흐려져…남부지방 옅은 황사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45656</th>\n",
       "      <td>내년부터 국가RD 평가 때 논문건수는 반영 않는다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45657</th>\n",
       "      <td>김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45658</th>\n",
       "      <td>회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45659</th>\n",
       "      <td>야외서 생방송 하세요…액션캠 전용 요금제 잇따라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45660</th>\n",
       "      <td>월드컵 태극전사 16강 전초기지 레오강 입성종합</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45661</th>\n",
       "      <td>미세먼지 속 출근길</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45662</th>\n",
       "      <td>왓츠앱稅 230원에 성난 레바논 민심…총리사퇴로 이어져종합2보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45663</th>\n",
       "      <td>베트남 경제 고성장 지속…2분기 GDP 6.71% 성장</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45664</th>\n",
       "      <td>그리스서 한국전 참전 기념식…참전용사 한반도 평화 기원</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45665</th>\n",
       "      <td>정진석 이정현 당현실 냉정하게 봐야…물러나는게 좋다종합</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45666</th>\n",
       "      <td>美 베네수엘라 구호품 반입 촉구 안보리 결의 추진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45667</th>\n",
       "      <td>황재균 쐐기타…kt 갈 길 바쁜 삼성에 고춧가루</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45668</th>\n",
       "      <td>정상회담 D1 文대통령 취임 후 남북관계 주요 일지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45669</th>\n",
       "      <td>LGU＋ 1분기 영업익 1천706억원…마케팅 비용 감소종합</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45670</th>\n",
       "      <td>박원순 시장 아부다비 루브르 박물관 방문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45671</th>\n",
       "      <td>방심위 강릉서 고성 산불현장처럼 보도한 KBS 관계자 징계</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45672</th>\n",
       "      <td>파키스탄 경제난 속 카타르서 30억 달러 투자 유치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45673</th>\n",
       "      <td>일본 예산팽창에 재정건전성 우려…내년에 장기채무 GDP 2배</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title\n",
       "index                                    \n",
       "45654          유튜브 내달 2일까지 크리에이터 지원 공간 운영\n",
       "45655             어버이날 맑다가 흐려져…남부지방 옅은 황사\n",
       "45656         내년부터 국가RD 평가 때 논문건수는 반영 않는다\n",
       "45657     김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것\n",
       "45658      회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간\n",
       "45659          야외서 생방송 하세요…액션캠 전용 요금제 잇따라\n",
       "45660          월드컵 태극전사 16강 전초기지 레오강 입성종합\n",
       "45661                          미세먼지 속 출근길\n",
       "45662  왓츠앱稅 230원에 성난 레바논 민심…총리사퇴로 이어져종합2보\n",
       "45663      베트남 경제 고성장 지속…2분기 GDP 6.71% 성장\n",
       "45664      그리스서 한국전 참전 기념식…참전용사 한반도 평화 기원\n",
       "45665      정진석 이정현 당현실 냉정하게 봐야…물러나는게 좋다종합\n",
       "45666         美 베네수엘라 구호품 반입 촉구 안보리 결의 추진\n",
       "45667          황재균 쐐기타…kt 갈 길 바쁜 삼성에 고춧가루\n",
       "45668        정상회담 D1 文대통령 취임 후 남북관계 주요 일지\n",
       "45669    LGU＋ 1분기 영업익 1천706억원…마케팅 비용 감소종합\n",
       "45670              박원순 시장 아부다비 루브르 박물관 방문\n",
       "45671    방심위 강릉서 고성 산불현장처럼 보도한 KBS 관계자 징계\n",
       "45672        파키스탄 경제난 속 카타르서 30억 달러 투자 유치\n",
       "45673   일본 예산팽창에 재정건전성 우려…내년에 장기채무 GDP 2배"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
